"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2257],{60522:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var t=s(74848),i=s(28453);const r={},o="File Processing",a={id:"nodejs/Nodejs File system",title:"File Processing",description:"Basic process with file",source:"@site/docs/nodejs/Nodejs File system.md",sourceDirName:"nodejs",slug:"/nodejs/Nodejs File system",permalink:"/docs/nodejs/Nodejs File system",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Node.js with WebAssembly",permalink:"/docs/nodejs/Node.js with WebAssembly"},next:{title:"Nodejs Monitor",permalink:"/docs/nodejs/Nodejs Monitor"}},c={},d=[];function l(e){const n={a:"a",code:"code",h1:"h1",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"File-Processing",children:"File Processing"}),"\n",(0,t.jsx)(n.p,{children:"Basic process with file"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-node",children:'const fs = require("fs"); \n  \n// Storing the JSON format data in myObject \nvar data = fs.readFileSync("data.json"); \nvar myObject = JSON.parse(data); \n  \n// Defining new data to be added \nlet newData = { \n  country: "England", \n}; \n  \n// Adding the new data to our object \nmyObject.push(newData); \n  \n// Writing to our JSON file \nvar newData2 = JSON.stringify(myObject); \nfs.writeFile("data2.json", newData2, (err) => { \n  // Error checking \n  if (err) throw err; \n  console.log("New data added"); \n}); \n'})}),"\n",(0,t.jsxs)(n.p,{children:["Processing huge JSON files\n",(0,t.jsx)(n.code,{children:"stream-json"}),"\n",(0,t.jsx)(n.a,{href:"https://github.com/uhop/stream-json",children:(0,t.jsx)(n.code,{children:"stream-json"})})," is a micro-library of node.js stream components with minimal dependencies for creating custom data processors oriented on processing huge JSON files while requiring a minimal memory footprint. It can parse JSON files far exceeding available memory. Even individual primitive data items (keys, strings, and numbers) can be streamed piece-wise. Streaming SAX-inspired event-based API is included as well."]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var t=s(96540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);